{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Машинное обучение, ФКН ВШЭ\n",
    "\n",
    "## Практическое задание 3. Градиентный спуск своими руками\n",
    "\n",
    "### Общая информация\n",
    "Дата выдачи: **23.10.2023**\n",
    "\n",
    "Мягкий дедлайн: **08.11.23 23:59**\n",
    "\n",
    "Жесткий дедлайн: **12.11.23 23:59**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### О задании\n",
    "\n",
    "В данном задании необходимо реализовать обучение линейной регрессии с помощью различных вариантов градиентного спуска.\n",
    "\n",
    "\n",
    "### Оценивание и штрафы\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов + 2 балла бонус.\n",
    "\n",
    "Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке.\n",
    "Также оценка может быть снижена за плохо читаемый код и плохо считываемые диаграммы.\n",
    "\n",
    "Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "\n",
    "### Формат сдачи\n",
    "Задания сдаются через систему Anytask. Инвайт можно найти на странице курса. Присылать необходимо ноутбук с выполненным заданием. Сам ноутбук называйте в формате homework-practice-03-gd-Username.ipynb, где Username — ваша фамилия.\n",
    "\n",
    "Для удобства проверки самостоятельно посчитайте свою максимальную оценку (исходя из набора решенных задач) и укажите ниже.\n",
    "\n",
    "**Оценка**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Напомним, что на лекциях и семинарах мы разбирали некоторые подходы к оптимизации функционалов по параметрам. В частности, был рассмотрен градиентный спуск и различные подходы к его реализации — полный градиентный спуск, стохастический градиентный спуск, метод импульса и другие. В качестве модели у нас будет выступать линейная регрессия.\n",
    "\n",
    "В этом домашнем задании вам предстоит реализовать 4 различных вариации градиентного спуска, написать свою реализацию линейной регресии, сравнить методы градиентного спуска между собой на реальных данных и ещё много чего веселого и интересного!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 1. Реализация градиентного спуска (3.5 балла)\n",
    "\n",
    "В этом задании вам предстоит написать собственные реализации различных подходов к градиентному спуску с опорой на подготовленные шаблоны в файле `descents.py`.\n",
    "\n",
    "**Все реализуемые методы должны быть векторизованы!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Лирическое-теоретическое отступление № 1\n",
    "\n",
    "Основное свойство антиградиента &ndash; он указывает в сторону наискорейшего убывания функции в данной точке. Соответственно, будет логично стартовать из некоторой точки, сдвинуться в сторону антиградиента,\n",
    "пересчитать антиградиент и снова сдвинуться в его сторону и т.д. Запишем это более формально.\n",
    "\n",
    "Пусть $w_0$ &ndash; начальный набор параметров (например, нулевой или сгенерированный из некоторого\n",
    "случайного распределения). Тогда ванильный градиентный спуск состоит в повторении следующих шагов до сходимости:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Лирическое-теоретическое отступление № 2\n",
    "\n",
    "На семинаре про [матрично-векторное дифференцирование](https://github.com/esokolov/ml-course-hse/blob/master/2022-fall/seminars/sem03-vector-diff.pdf) вы должны были обсуждать дифференцирование функции потерь MSE в матричном виде.\n",
    "\n",
    "### Задание 1.0. Градиент MSE в матричном виде (0 баллов).\n",
    "\n",
    "Напомним, что функция потерь MSE записывается в матричном виде как:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\dfrac{1}{\\ell} \\left( y - Xw \\right)^T \\left( y - Xw \\right)\n",
    "$$\n",
    "\n",
    "Выпишите ниже (подсмотрев в семинар или решив самостоятельно) градиент для функции потерь MSE в матричном виде."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$$\\nabla_{w} Q(w) = 2X^T(Xw - y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задание 1.1. Родительский класс BaseDescent (0.5 балла).\n",
    "\n",
    "Реализуйте функции `calc_loss` (вычисление MSE для переданных $x$ и $y$) и `predict` (предсказание $y_{pred}$ для переданных $x$) в классе `BaseDescent`.\n",
    "\n",
    "Все вычисления должны быть векторизованы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задание 1.2. Полный градиентный спуск VanillaGradientDescent (0.5 балла).\n",
    "\n",
    "Реализуйте полный градиентный спуск заполнив пропуски в классе `VanillaGradientDescent` в файле `descents.py`. Для вычисления градиента используйте формулу выше. Шаг оптимизации:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "Здесь и далее функция `update_weights` должна возвращать разницу между $w_{k + 1}$ и $w_{k}$: $\\quad w_{k + 1} - w_{k} = -\\eta_{k} \\nabla_{w} Q(w_{k})$.\n",
    "\n",
    "Во всех методах градиентного спуска мы будем использовать следующую формулу для длины шага:\n",
    "\n",
    "$$\n",
    "    \\eta_{k} = \\lambda \\left(\\dfrac{s_0}{s_0 + k}\\right)^p\n",
    "$$\n",
    "\n",
    "На практике достаточно настроить параметр $\\lambda$, а остальным выставить параметры по умолчанию: $s_0 = 1, \\, p = 0.5.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Лирическое-теоретическое отступление № 3\n",
    "\n",
    "Как правило, в задачах машинного обучения функционал $Q(w)$ представим в виде суммы $\\ell$ функций:\n",
    "\n",
    "$$\n",
    "    Q(w)\n",
    "    =\n",
    "    \\frac{1}{\\ell}\n",
    "    \\sum_{i = 1}^{\\ell}\n",
    "        q_i(w).\n",
    "$$\n",
    "\n",
    "В нашем домашнем задании отдельные функции $q_i(w)$ соответствуют ошибкам на отдельных объектах.\n",
    "\n",
    "Проблема метода градиентного спуска состоит в том, что на каждом шаге необходимо вычислять градиент всей суммы (будем его называть полным градиентом):\n",
    "\n",
    "$$\n",
    "    \\nabla_w Q(w)\n",
    "    =\n",
    "    \\frac{1}{\\ell}\n",
    "    \\sum_{i = 1}^{\\ell}\n",
    "        \\nabla_w q_i(w).\n",
    "$$\n",
    "\n",
    "Это может быть очень трудоёмко при больших размерах выборки. В то же время точное вычисление градиента может быть не так уж необходимо &ndash; как правило, мы делаем не очень большие шаги в сторону антиградиента, и наличие в нём неточностей не должно сильно сказаться на общей траектории.\n",
    "\n",
    "Оценить градиент суммы функций можно средним градиентов случайно взятого подмножества функций:\n",
    "\n",
    "$$\n",
    "    \\nabla_{w} Q(w_{k}) \\approx \\dfrac{1}{|B|}\\sum\\limits_{i \\in B}\\nabla_{w} q_{i}(w_{k}),\n",
    "$$\n",
    "где $B$ - это случайно выбранное подмножество индексов.\n",
    "\n",
    "В этом случае мы получим метод **стохастического градиентного спуска**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задание 1.3. Стохастический градиентный спуск StochasticDescent (0.5 балла).\n",
    "\n",
    "Реализуйте стохастический градиентный спуск заполнив пропуски в классе `StochasticDescent`. Для оценки градиента используйте формулу выше (среднее градиентов случайно выбранного батча объектов). Шаг оптимизации:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\dfrac{1}{|B|}\\sum\\limits_{i \\in B}\\nabla_{w} q_{i}(w_{k}).\n",
    "$$\n",
    "\n",
    "Размер батча будет являться гиперпараметром метода, семплируйте индексы для батча объектов с помощью `np.random.randint`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Лирическое-теоретическое отступление № 4\n",
    "\n",
    "Может оказаться, что направление антиградиента сильно меняется от шага к шагу. Например, если линии уровня функционала сильно вытянуты, то из-за ортогональности градиента линиям уровня он будет менять направление на почти противоположное на каждом шаге. Такие осцилляции будут вносить сильный шум в движение, и процесс оптимизации займёт много итераций. Чтобы избежать этого, можно усреднять векторы антиградиента с нескольких предыдущих шагов &ndash; в этом случае шум уменьшится, и такой средний вектор будет указывать в сторону общего направления движения. Введём для этого вектор инерции:\n",
    "\n",
    "\\begin{align}\n",
    "    &h_0 = 0, \\\\\n",
    "    &h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_w Q(w_{k})\n",
    "\\end{align}\n",
    "\n",
    "Здесь $\\alpha$ &ndash; параметр метода, определяющей скорость затухания градиентов с предыдущих шагов. Разумеется, вместо вектора градиента может быть использована его аппроксимация. Чтобы сделать шаг градиентного спуска, просто сдвинем предыдущую точку на вектор инерции:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "Заметим, что если по какой-то координате градиент постоянно меняет знак, то в результате усреднения градиентов в векторе инерции эта координата окажется близкой к нулю. Если же по координате знак градиента всегда одинаковый, то величина соответствующей координаты в векторе инерции будет большой, и мы будем делать большие шаги в соответствующем направлении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задание 1.4 Метод Momentum MomentumDescent (0.5 балла).\n",
    "\n",
    "Реализуйте градиентный спуск с методом инерции заполнив пропуски в классе `MomentumDescent`. Шаг оптимизации:\n",
    "\n",
    "\\begin{align}\n",
    "    &h_0 = 0, \\\\\n",
    "    &h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_w Q(w_{k}) \\\\\n",
    "    &w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "\\end{align}\n",
    "\n",
    "$\\alpha$ будет являться гиперпараметром метода, но в данном домашнем задании мы зафиксируем её за вас $\\alpha = 0.9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Лирическое-теоретическое отступление № 5\n",
    "\n",
    "Градиентный спуск очень чувствителен к выбору длины шага. Если шаг большой, то есть риск, что мы будем перескакивать через точку минимума; если же шаг маленький, то для нахождения минимума потребуется много итераций. При этом нет способов заранее определить правильный размер шага &ndash; к тому же, схемы с постепенным уменьшением шага по мере итераций могут тоже плохо работать.\n",
    "\n",
    "В методе AdaGrad предлагается сделать свою длину шага для каждой компоненты вектора параметров. При этом шаг будет тем меньше, чем более длинные шаги мы делали на предыдущих итерациях:\n",
    "\n",
    "\\begin{align}\n",
    "    &G_{kj} = G_{k-1,j} + (\\nabla_w Q(w_{k - 1}))_j^2; \\\\\n",
    "    &w_{jk} = w_{j,k-1} - \\frac{\\eta_t}{\\sqrt{G_{kj}} + \\varepsilon} (\\nabla_w Q(w_{k - 1}))_j.\n",
    "\\end{align}\n",
    "\n",
    "Здесь $\\varepsilon$ небольшая константа, которая предотвращает деление на ноль.\n",
    "\n",
    "В данном методе можно зафксировать длину шага (например, $\\eta_k = 0.01$) и не подбирать её в процессе обучения. Отметим, что данный метод подходит для разреженных задач, в которых у каждого объекта большинство признаков равны нулю. Для признаков, у которых ненулевые значения встречаются редко, будут делаться большие шаги; если же какой-то признак часто является ненулевым, то шаги по нему будут небольшими.\n",
    "\n",
    "У метода AdaGrad есть большой недостаток: переменная $G_{kj}$ монотонно растёт, из-за чего шаги становятся всё медленнее и могут остановиться ещё до того, как достигнут минимум функционала. Проблема решается в методе RMSprop, где используется экспоненциальное затухание градиентов:\n",
    "\n",
    "$$\n",
    "    G_{kj} = \\alpha G_{k-1,j} + (1 - \\alpha) (\\nabla_w Q(w^{(k-1)}))_j^2.\n",
    "$$\n",
    "\n",
    "В этом случае размер шага по координате зависит в основном от того, насколько\n",
    "быстро мы двигались по ней на последних итерациях.\n",
    "\n",
    "Можно объединить идеи описанных выше методов: накапливать градиенты со всех прошлых шагов для\n",
    "избежания осцилляций и делать адаптивную длину шага по каждому параметру."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задание 1.5. Метод Adam (Adaptive Moment Estimation) (1.5 балла).\n",
    "\n",
    "![](adam_meme.png)\n",
    "\n",
    "Реализуйте градиентный спуск с методом Adam заполнив пропуски в классе `Adam`. Шаг оптимизации:\n",
    "\n",
    "\\begin{align}\n",
    "    &m_0 = 0, \\quad v_0 = 0; \\\\ \\\\\n",
    "    &m_{k + 1} = \\beta_1 m_k + (1 - \\beta_1) \\nabla_w Q(w_{k}); \\\\ \\\\\n",
    "    &v_{k + 1} = \\beta_2 v_k + (1 - \\beta_2) \\left(\\nabla_w Q(w_{k})\\right)^2; \\\\ \\\\\n",
    "    &\\widehat{m}_{k} = \\dfrac{m_k}{1 - \\beta_1^{k}}, \\quad \\widehat{v}_{k} = \\dfrac{v_k}{1 - \\beta_2^{k}}; \\\\ \\\\\n",
    "    &w_{k + 1} = w_{k} - \\dfrac{\\eta_k}{\\sqrt{\\widehat{v}_{k + 1}} + \\varepsilon} \\widehat{m}_{k + 1}.\n",
    "\\end{align}\n",
    "\n",
    "$\\beta_1 = 0.9, \\beta_2 = 0.999$ и $\\varepsilon = 10^{-8}$ будут зафиксированы за вас."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 2. Реализация линейной регресии (0.5 балла)\n",
    "\n",
    "В этом задании вам предстоит написать свою реализацию линейной регресии, обучаемой с использованием градиентного спуска, с опорой на подготовленные шаблоны в файле `linear_regression.py` - **LinearRegression**. По сути линейная регрессия будет оберткой, которая запускает обучение \n",
    "\n",
    "Необходимо соблюдать следующие условия:\n",
    "\n",
    "* Все вычисления должны быть векторизованы;\n",
    "* Циклы средствами python допускаются только для итераций градиентного спуска;\n",
    "* В качестве критерия останова необходимо использовать (одновременно):\n",
    "    * Квадрат евклидовой нормы разности весов на двух соседних итерациях меньше `tolerance`;\n",
    "    * Разность весов содержит наны;\n",
    "    * Достижение максимального числа итераций `max_iter`.\n",
    "* Будем считать, что все данные, которые поступают на вход имеют столбец единичек последним столбцом;\n",
    "* Чтобы проследить за сходимостью оптимизационного процесса будем использовать `loss_history`, в нём будем хранить значения функции потерь до каждого шага, начиная с нулевого (до первого шага по антиградиенту) и значение функции потерь после оптимизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 3. Проверка кода (0 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from descents import get_descent\n",
    "from linear_regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_objects = 100\n",
    "dimension = 5\n",
    "\n",
    "x = np.random.rand(num_objects, dimension)\n",
    "y = np.random.rand(num_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Descents\n",
    "\n",
    "descent_config = {\n",
    "    'descent_name': 'some name that we will replace in the future',\n",
    "    'kwargs': {\n",
    "        'dimension': dimension\n",
    "    }\n",
    "}\n",
    "\n",
    "for descent_name in ['full', 'stochastic', 'momentum', 'adam']:\n",
    "    descent_config['descent_name'] = descent_name\n",
    "    descent = get_descent(descent_config)\n",
    "\n",
    "    diff = descent.step(x, y)\n",
    "    gradient = descent.calc_gradient(x, y)\n",
    "    predictions = descent.predict(x)\n",
    "\n",
    "    assert gradient.shape[0] == dimension, f'Gradient failed for descent {descent_name}'\n",
    "    assert diff.shape[0] == dimension, f'Weights failed for descent {descent_name}'\n",
    "    assert predictions.shape == y.shape, f'Prediction failed for descent {descent_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# LinearRegression\n",
    "\n",
    "max_iter = 10\n",
    "tolerance = 0\n",
    "\n",
    "descent_config = {\n",
    "    'descent_name': 'stochastic',\n",
    "    'kwargs': {\n",
    "        'dimension': dimension,\n",
    "        'batch_size': 10\n",
    "    }\n",
    "}\n",
    "\n",
    "regression = LinearRegression(\n",
    "    descent_config=descent_config,\n",
    "    tolerance=tolerance,\n",
    "    max_iter=max_iter\n",
    ")\n",
    "\n",
    "regression.fit(x, y)\n",
    "\n",
    "assert len(regression.loss_history) == max_iter + 1, 'Loss history failed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Если ваше решение прошло все тесты локально, то теперь пришло время протестировать его в [Яндекс Контесте](https://contest.yandex.ru/contest/54610/).\n",
    "\n",
    "Для каждой задачи из контеста вставьте ссылку на успешную посылку:\n",
    "\n",
    "* **VanillaGradientDescent**: [96883556](https://contest.yandex.ru/contest/54610/run-report/96883556/)\n",
    "\n",
    "\n",
    "* **StochasticDescent**: [96883576](https://contest.yandex.ru/contest/54610/run-report/96883576/)\n",
    "\n",
    "\n",
    "* **MomentumDescent**: [96883592](https://contest.yandex.ru/contest/54610/run-report/96883592/)\n",
    "\n",
    "\n",
    "* **Adam**: [96885440](https://contest.yandex.ru/contest/54610/run-report/96885440/)\n",
    "\n",
    "\n",
    "* **LinearRegression**: [96885053](https://contest.yandex.ru/contest/54610/run-report/96885053/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 4. Работа с данными (1 балл)\n",
    "\n",
    "Мы будем использовать датасет объявлений по продаже машин на немецком Ebay. В задаче предсказания целевой переменной для нас будет являться цена."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Постройте график распределения целевой переменной в данных, подумайте, нужно ли заменить её на логарифм. Присутствуют ли выбросы в данных с аномальной ценой? Если да, то удалите их из данных.\n",
    "\n",
    "* Проведите исследование данных:\n",
    "    * Проанализируйте тип столбцов, постройте графики зависимости целевой переменной от признака, распределения значений признака;\n",
    "    * Подумайте, какие признаки могут быть полезными на основе этих графиков, обработайте выбросы;\n",
    "    * Подумайте, какие трансформации признаков из известных вам будет уместно применить;\n",
    "    * Разделите полезные признаки на категориальные, вещественные и те, которые не надо предобрабатывать.\n",
    "* Разделите данные на обучающую, валидационную и тестовую выборки в отношении 8:1:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from descents import get_descent\n",
    "from linear_regression import LinearRegression\n",
    "\n",
    "sns.set(style='darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('autos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Колонки в данных:\n",
    "\n",
    "* `brand` - название бренда автомобиля\n",
    "* `model` - название модели автомобиля\n",
    "* `vehicleType` - тип транспортного средства\n",
    "* `gearbox` - тип трансмисcии\n",
    "* `fuelType` - какой вид топлива использует автомобиль\n",
    "* `notRepairedDamage` - есть ли в автомобиле неисправность, которая еще не устранена\n",
    "* `powerPS` - мощность автомобиля в PS (метрическая лошадиная сила)\n",
    "* `kilometer` - сколько километров проехал автомобиль, пробег\n",
    "* `autoAgeMonths` - возраст автомобиля в месяцах\n",
    "\n",
    "\n",
    "* `price` - цена, указанная в объявлении о продаже автомобиля (целевая переменная)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data, x=\"price\", kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data[data.price > 40000], x=\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data[data.price < 1000], x=\"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу можем удалить машины стоимостью свыше 90к - это какой-то супер премиальный класс и их там по пальцам пересчитать, а также машины стоимостью 0. Хорошая идея здесь - сразу подумать либо о разделении машин на классы премиальности, чтобы сделать отдельную модель для каждого, либо о хорошей фильтрации датасета под задачу - мы автодилер дорогих авто или же мы авторынок вёдер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data.price > 0) & (data.price < 90000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data, x=\"price\", kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть тяжелый хвост, но выкидывать его просто так - не самая хорошая идея. А еще распределение очень похоже на лог-нормальное - проверим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"log_price\"] = np.log1p(data[\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data, x=\"log_price\", kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виден тяжелый хвост слева - печаль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data[data.log_price < 5], x=\"log_price\", kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data[data.price < 1000], x=\"price\", kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это тоже что-то странное - еще чуть обрежем хвост"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.price > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data, x=\"log_price\", kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так-то лучше!\n",
    "\n",
    "Теперь заметим, что если мы будем обучать на цене, то вряд ли будем точны при предсказании дорогих машин - а так как мы не знаем точную задачу, то их мы тоже хотим предсказывать. Наверное, внутренняя логика подсказывает, что логичнее было бы предиктить цену, но вот разумный машин-лернер внутри меня говорит, что строить модели на лог-нормально распределении == забивать на большие значения или наоборот переобучаться под них в зависимости от функции потерь. А значит будем предиктить log_price или что то же самое, как было описано в прошлой дз, обучаться под RMSLE цены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.loc[data[\"notRepairedDamage\"] == \"ja\", [\"notRepairedDamage\"]] = 1.0\n",
    "data.loc[data[\"notRepairedDamage\"] == \"nein\", [\"notRepairedDamage\"]] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот теперь разделим выборку на train+val и test, чтобы не видеть паттерны в тестовой выборке и не подгоняться под них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, test_size=0.2, random_state=42, shuffle=True)\n",
    "data_val, data_test = train_test_split(data_test, test_size=0.5, shuffle=False)\n",
    "len(data_train), len(data_val), len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data_train, x=\"log_price\", kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "\n",
    "for (i, feature) in enumerate([\"vehicleType\", \"fuelType\", \"gearbox\"]):\n",
    "    axes[i].set_title(f\"LogPrice distribution over {feature}\")\n",
    "    sns.boxplot(data_train, hue=feature, y=\"log_price\", ax=axes[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.loc[data[\"gearbox\"] == \"automatik\", [\"automatic_gearbox\"]] = 1.0\n",
    "data_train.loc[data[\"gearbox\"] == \"manuell\", [\"automatic_gearbox\"]] = 0.0\n",
    "\n",
    "data_test.loc[data[\"gearbox\"] == \"automatik\", [\"automatic_gearbox\"]] = 1.0\n",
    "data_test.loc[data[\"gearbox\"] == \"manuell\", [\"automatic_gearbox\"]] = 0.0\n",
    "\n",
    "data_val.loc[data[\"gearbox\"] == \"automatik\", [\"automatic_gearbox\"]] = 1.0\n",
    "data_val.loc[data[\"gearbox\"] == \"manuell\", [\"automatic_gearbox\"]] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут видны действительно согласующиеся с реальностью закономерность - электро и гибриды дороже рынка, бензиновые - самые дешевые. Аналогично малолитражные легковушки - самые дешевые, а вот suv'ы - самые дорогие.\n",
    "Здесь мало классов - достаточно будет OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "grouped = data_train.loc[:,['brand', 'log_price']] \\\n",
    "    .groupby(['brand']) \\\n",
    "    .median() \\\n",
    "    .sort_values(by='log_price')\n",
    "sns.boxplot(data_train, y=\"brand\", x=\"log_price\", order=grouped.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 40))\n",
    "grouped = data_train.loc[:,['model', 'log_price']] \\\n",
    "    .groupby(['model']) \\\n",
    "    .median() \\\n",
    "    .sort_values(by='log_price')\n",
    "sns.boxplot(data_train, y=\"model\", x=\"log_price\", order=grouped.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извиняюсь, что без цвета - sns почему-то без hue не хочет делать цветные столбики((("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут тоже можно заметить, что есть дорогие марки, а есть - дешевые и аналогично с моделью. Еще можно сразу заметить, что по признаку марки у нас нет гетероскедастичности - дисперсия цены дорогих марок значительно ниже дисперсии цены дешевых - это немного испортит модель. Еще есть гипотеза, что модель + пробег == прекрасная модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(data_train.brand.values)), len(np.unique(data_train.model.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут у нас уже довольно много признаков - хорошо бы использовать MTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь глянем на численные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(10, 20))\n",
    "\n",
    "for (i, feature) in enumerate([\"powerPS\", \"kilometer\", \"autoAgeMonths\"]):\n",
    "    axes[i].set_title(f\"LogPrice dependence over {feature}\")\n",
    "    sns.scatterplot(data_train, x=feature, y=\"log_price\", ax=axes[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут можно заметить два факта: пробег вообще говоря дискретный (если быть точнее - порядковый) и точек слишком много - такая отрисовка дает нам ровным счетом ничего("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_sorted = np.unique(np.sort(data_train.kilometer.values)).astype(\"str\")\n",
    "km_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[\"km_str\"] = data_train.kilometer.astype(\"str\")\n",
    "data_test[\"km_str\"] = data_test.kilometer.astype(\"str\")\n",
    "data_val[\"km_str\"] = data_val.kilometer.astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.boxplot(data_train, y=\"km_str\", x=\"log_price\", order=km_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data_train, y=\"kilometer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скорее всего, в 150к входит 150+, от этого там так много. Видно, что есть обратная зависимость - можно оставить как порядковую фичу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=data_train.groupby([\"powerPS\"]).agg({\"log_price\" : \"mean\"}).rename(columns={\"log_price\" : \"mean_log_price\"}),\n",
    "    x=\"powerPS\",\n",
    "    y=\"mean_log_price\",\n",
    "    kind=\"line\",\n",
    "    aspect=1.5\n",
    ").set(\n",
    "    title=\"Dependence of Mean LogPrice on powerPS\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "sns.relplot(\n",
    "    data=data_train.groupby([\"autoAgeMonths\"]).agg({\"log_price\" : \"mean\"}).rename(columns={\"log_price\" : \"mean_log_price\"}),\n",
    "    x=\"autoAgeMonths\",\n",
    "    y=\"mean_log_price\",\n",
    "    kind=\"line\",\n",
    "    aspect=1.5\n",
    ").set(\n",
    "    title=\"Dependence of Mean LogPrice on autoAgeMonths\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_train[data_train.autoAgeMonths > 240]) / len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[\"autoAgeMonthsTuned\"] = np.abs(data_train[\"autoAgeMonths\"] - 240)\n",
    "sns.relplot(\n",
    "    data=data_train.groupby([\"autoAgeMonthsTuned\"]).agg({\"log_price\" : \"mean\"}).rename(columns={\"log_price\" : \"mean_log_price\"}),\n",
    "    x=\"autoAgeMonthsTuned\",\n",
    "    y=\"mean_log_price\",\n",
    "    kind=\"line\",\n",
    "    aspect=1.5\n",
    ").set(\n",
    "    title=\"Dependence of Mean LogPrice on autoAgeMonthsTuned\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "data_train[\"autoAgeMonthsTuned\"] = np.abs(data_train[\"autoAgeMonths\"] - 310)\n",
    "sns.relplot(\n",
    "    data=data_train.groupby([\"autoAgeMonthsTuned\"]).agg({\"log_price\" : \"mean\"}).rename(columns={\"log_price\" : \"mean_log_price\"}),\n",
    "    x=\"autoAgeMonthsTuned\",\n",
    "    y=\"mean_log_price\",\n",
    "    kind=\"line\",\n",
    "    aspect=1.5\n",
    ").set(\n",
    "    title=\"Dependence of Mean LogPrice on autoAgeMonthsTuned\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно, тут есть граница между \"старое ведро\" и \"ооо, раритет\" - 20 лет. А еще снова гетероскедастичность(\n",
    "Можно было бы сделать $|feature - x|$ и работать с такой величиной, нооо гетероскедастичность мешает нам - дисперсия раритетов выше и зависимость чуть более пологая - поэтому решение такое: разделим эту фичу на две: до 240 и свыше 240 - остальное занулим. Конечно, после шкалирования нули сместятся, но, я верю, что нас спасет свободный член\n",
    "\n",
    "С мощностью все ок - в идеале модель должна выучить тренд, а все остальное - это оставшиеся фичи + шум"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[\"autoStandardAgeMonths\"] = data_train[\"autoAgeMonths\"] * (data_train[\"autoAgeMonths\"] <= 240)\n",
    "data_train[\"autoRareAgeMonths\"] = data_train[\"autoAgeMonths\"] * (data_train[\"autoAgeMonths\"] > 240)\n",
    "\n",
    "data_test[\"autoStandardAgeMonths\"] = data_test[\"autoAgeMonths\"] * (data_test[\"autoAgeMonths\"] <= 240)\n",
    "data_test[\"autoRareAgeMonths\"] = data_test[\"autoAgeMonths\"] * (data_test[\"autoAgeMonths\"] > 240)\n",
    "\n",
    "data_val[\"autoStandardAgeMonths\"] = data_val[\"autoAgeMonths\"] * (data_val[\"autoAgeMonths\"] <= 240)\n",
    "data_val[\"autoRareAgeMonths\"] = data_val[\"autoAgeMonths\"] * (data_val[\"autoAgeMonths\"] > 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=data_train.groupby([\"autoStandardAgeMonths\"]).agg({\"log_price\" : \"mean\"}).rename(columns={\"log_price\" : \"mean_log_price\"}),\n",
    "    x=\"autoStandardAgeMonths\",\n",
    "    y=\"mean_log_price\",\n",
    "    kind=\"line\",\n",
    "    aspect=1.5\n",
    ").set(\n",
    "    title=\"Dependence of Mean LogPrice on autoStandardAgeMonths\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "sns.relplot(\n",
    "    data=data_train.groupby([\"autoRareAgeMonths\"]).agg({\"log_price\" : \"mean\"}).rename(columns={\"log_price\" : \"mean_log_price\"}),\n",
    "    x=\"autoRareAgeMonths\",\n",
    "    y=\"mean_log_price\",\n",
    "    kind=\"line\",\n",
    "    aspect=1.5\n",
    ").set(\n",
    "    title=\"Dependence of Mean LogPrice on autoRareAgeMonths\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нули модели не помеха, как говорится"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и последнее - бинарная фича про восстановленные машины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data_train, hue=\"notRepairedDamage\", y=\"log_price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут, как и ожидалось, все ок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_ohe = ['brand', 'model']\n",
    "# categorical_mte = ['vehicleType', 'fuelType']\n",
    "# ordinal = [\"km_str\"]\n",
    "# numeric = [\"powerPS\", \"autoStandardAgeMonths\", \"autoRareAgeMonths\"]\n",
    "# other = [\"notRepairedDamage\", \"automatic_gearbox\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, наш самописный линрег взрывается на lr > 1e-1, если кормить ему нешкалированные фичи, но библиотечному норм((\\\n",
    "Поэтому мне придется остановиться на ohe, numeric и other(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_ohe = ['brand', 'model', 'vehicleType', 'fuelType', \"km_str\"]\n",
    "numeric = [\"powerPS\", \"autoStandardAgeMonths\", \"autoRareAgeMonths\"]\n",
    "other = [\"notRepairedDamage\", \"automatic_gearbox\"]\n",
    "ordinal = []\n",
    "categorical_mte = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_train['bias'] = 1\n",
    "data_test['bias'] = 1\n",
    "data_val['bias'] = 1\n",
    "other += ['bias']\n",
    "\n",
    "x_train = data_train[categorical_ohe + categorical_mte + numeric + ordinal + other]\n",
    "y_train = data_train['log_price'].to_numpy()\n",
    "x_test = data_test[categorical_ohe + categorical_mte + numeric + ordinal + other]\n",
    "y_test = data_test['log_price'].to_numpy()\n",
    "x_val = data_val[categorical_ohe + categorical_mte + numeric + ordinal + other]\n",
    "y_val = data_val['log_price'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'), categorical_ohe),\n",
    "    ('mte', TargetEncoder(target_type='continuous', shuffle=False), categorical_mte),\n",
    "    ('ord', OrdinalEncoder(categories=[km_sorted]), ordinal),\n",
    "    ('scaling', StandardScaler(), numeric),\n",
    "    ('other',  'passthrough', other)\n",
    "])\n",
    "\n",
    "x_train = column_transformer.fit_transform(x_train, y_train).astype(np.float64)\n",
    "x_test = column_transformer.transform(x_test).astype(np.float64)\n",
    "x_val = column_transformer.transform(x_val).astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ура, 3 часа на эту часть - надеюсь, я не получу скор 183453745134\n",
    "\n",
    "Так я думал пару часов назад, пока у меня не стали взрываться градиенты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "linreg = LR()\n",
    "linreg.fit(x_train, y_train)\n",
    "MSE(linreg.predict(x_val), y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удивительным образом - это лучше всего того, что я там крутил с mte и ordinal. Мораль и так ясна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 5. Сравнение методов градиентного спуска (2 балла)\n",
    "\n",
    "В этом задании вам предстоит сравнить методы градиентного спуска на подготовленных вами данных из предыдущего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задание 5.1. Подбор оптимальной длины шага (1 балл)\n",
    "\n",
    "Подберите по валидационной выборке наилучшую длину шага $\\lambda$ для каждого метода с точки зрения ошибки. Для этого сделайте перебор по логарифмической сетке. Для каждого метода посчитайте ошибку на обучающей и тестовой выборках, посчитайте качество по метрике $R^2$, сохраните количество итераций до сходимости.\n",
    "\n",
    "Все параметры кроме `lambda_` стоит выставить равным значениям по умолчанию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_update_stats(model, x_train, y_train, x_val, y_val, stats, name):\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    stats[name][\"losses_history\"].append(model.loss_history)\n",
    "    stats[name][\"losses_train\"].append(model.loss_history[-1])\n",
    "    stats[name][\"r2_train\"].append(r2_score(y_train, model.predict(x_train)))\n",
    "\n",
    "    loss_val = model.calc_loss(x_val, y_val)\n",
    "    stats[name][\"losses_val\"].append(loss_val) \n",
    "    stats[name][\"r2_val\"].append(r2_score(y_val, model.predict(x_val))) \n",
    "\n",
    "    stats[name][\"iterations\"].append(len(model.loss_history) - 1)\n",
    "\n",
    "def create_default_dict():\n",
    "    return defaultdict(lambda: defaultdict(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descent_names = ['full', 'stochastic', 'momentum', 'adam']\n",
    "\n",
    "descent_config = {\n",
    "    'descent_name': 'name',\n",
    "    'kwargs': {\n",
    "        'dimension': x_train.shape[1],\n",
    "        'lambda_': 1e-3\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lambdas = [5, 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "stats = create_default_dict()\n",
    "\n",
    "for descent_name in descent_names:\n",
    "    descent_config['descent_name'] = descent_name\n",
    "    for lambda_ in lambdas:\n",
    "        descent_config['kwargs']['lambda_'] = lambda_\n",
    "        descent = get_descent(descent_config)\n",
    "\n",
    "        regression = LinearRegression(\n",
    "            descent_config=descent_config\n",
    "        )\n",
    "\n",
    "        fit_and_update_stats(regression, x_train, y_train, x_val, y_val, stats, descent_name)\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задание 5.2. Сравнение методов (1 балла) \n",
    "\n",
    "Постройте график зависимости ошибки на обучающей выборке от номера итерации (все методы на одном графике).\n",
    "\n",
    "Посмотрите на получившиеся результаты (таблички с метриками и график). Сравните методы между собой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним результаты обучений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_params(stats_dict, params, param_name, metrics_name=\"losses_val\"):\n",
    "    results = list()\n",
    "    for name, stats in stats_dict.items():\n",
    "        best_ind = np.argmin(stats[metrics_name])\n",
    "        best_stats = dict()\n",
    "        for stat_name, values in stats.items():\n",
    "            best_stats[stat_name] = values[best_ind]\n",
    "        best_stats[param_name] = params[best_ind]\n",
    "        best_stats[\"method_name\"] = name\n",
    "        results.append(best_stats)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = get_best_params(stats, lambdas, \"lambda\")   \n",
    "results_df = pd.DataFrame(results)\n",
    "results_df[[\"method_name\", \"losses_train\", \"losses_val\", \"r2_train\", \"r2_val\", \"iterations\", \"lambda\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in results:\n",
    "    plt.plot(res[\"losses_history\"], label=res[\"method_name\"])\n",
    "\n",
    "plt.title(\"Losses while fitting with best lambda\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in results:\n",
    "    plt.plot(res[\"losses_history\"][10:], label=res[\"method_name\"])\n",
    "\n",
    "plt.title(\"Losses while fitting with best lambda (from step 10)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in results:\n",
    "    plt.plot(res[\"losses_history\"][40:], label=res[\"method_name\"])\n",
    "\n",
    "plt.title(\"Losses while fitting with best lambda (from step 40)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Выводы:**\n",
    "\n",
    "1. Для всех методов выиграл большой шаг - это, скорее всего особенность рельефа функции потерь в нашей задаче, в дл, к примеру, чаще используется очень низкий lr\n",
    "2. У адама выиграла лямбда аж 5 - он умеет к такому приспосабливаться, не перепрыгивая через оптимум, по мере приближения к нему\n",
    "3. Стохастическому не хватило шагов - стандартный batch_size = 10 неплохо приближает функцию (градиент не разнесло), но не хватает для достаточно длинных шагов\n",
    "4. full сошелся быстрее всех по понятным причинам, но попал в локальным минимум\n",
    "5. Видно, как блуждают adam и momentum вдали от оптимума, причем adam делает это чуть реже как раз из-за адаптации\n",
    "6. Итого: самый лучший - adam с lambda = 5, самый быстросходящийся - full\n",
    "7. R^2 тоже подтверждает выводы, как показатель адекватности модели - в adam и momentum мы больше задействуем данные "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 6. Стохастический градиентный спуск и размер батча (1 балл)\n",
    "\n",
    "В этом задании вам предстоит исследовать влияние размера батча на работу стохастического градиентного спуска. \n",
    "\n",
    "* Сделайте по несколько запусков (например, k = 10) стохастического градиентного спуска на обучающей выборке для каждого размера батча из перебираемого списка. Замерьте время в секундах и количество итераций до сходимости. Посчитайте среднее этих значений для каждого размера батча.\n",
    "* Постройте график зависимости количества шагов до сходимости от размера батча.\n",
    "* Постройте график зависимости времени до сходимости от размера батча.\n",
    "\n",
    "Посмотрите на получившиеся результаты. Какие выводы можно сделать про подбор размера батча для стохастического градиентного спуска?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descent_config = {\n",
    "    'descent_name': 'stochastic',\n",
    "    'kwargs': {\n",
    "        'dimension': x_train.shape[1],\n",
    "        'lambda_': 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_sizes = np.arange(5, 500, 10)\n",
    "\n",
    "mean_time = []\n",
    "mean_iter = []\n",
    "mean_loss_val = []\n",
    "mean_loss_train = []\n",
    "\n",
    "for batch_size in tqdm(batch_sizes):\n",
    "    descent_config[\"kwargs\"][\"batch_size\"] = batch_size\n",
    "    time_spent = []\n",
    "    iterations = []\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    for _ in range(10):\n",
    "        descent = get_descent(descent_config)\n",
    "        regression = LinearRegression(\n",
    "            descent_config=descent_config,\n",
    "            max_iter=500\n",
    "        )\n",
    "\n",
    "        start_time = time()\n",
    "        regression.fit(x_train, y_train)\n",
    "        stop_time = time()\n",
    "\n",
    "        losses_train.append(regression.loss_history[-1])\n",
    "        loss_val = regression.calc_loss(x_val, y_val)\n",
    "        losses_val.append(loss_val) \n",
    "        iterations.append(len(regression.loss_history) - 1)\n",
    "        time_spent.append(stop_time - start_time)\n",
    "    mean_time.append(np.mean(time_spent))\n",
    "    mean_iter.append(np.mean(iterations))\n",
    "    mean_loss_val.append(np.mean(losses_val))\n",
    "    mean_loss_train.append(np.mean(losses_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(30, 20))\n",
    "\n",
    "ax[0].plot(mean_iter)\n",
    "ax[0].set_title(\"Relation between batch_size and number of iterations\")\n",
    "ax[0].set_xticks(ticks=range(len(batch_sizes)), labels=batch_sizes)\n",
    "\n",
    "ax[1].plot(mean_time)\n",
    "ax[1].set_title(\"Relation between batch_size and fitting time\")\n",
    "ax[1].set_xticks(ticks=range(len(batch_sizes)), labels=batch_sizes)\n",
    "\n",
    "ax[2].plot(mean_loss_val[1:], label=\"val_loss\")\n",
    "ax[2].plot(mean_loss_train[1:], label=\"train_loss\")\n",
    "ax[2].set_title(\"Relation between batch_size and loss\")\n",
    "ax[2].set_xticks(ticks=range(len(batch_sizes) - 1), labels=batch_sizes[1:])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Количество итераций и время обучения довольно быстро убывает и выходит на плато в 170 итераций и меньше 0.5 секунд при размере батча около 450\n",
    "2. Ошибка выходит на плато уже с размера батча 85, но при этом ошибка не уходит ниже 0.3 - скорее всего, нужен batch_size сильно больше "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 7. Регуляризация (1 балл)\n",
    "\n",
    "В этом задании вам предстоит исследовать влияние регуляризации на работу различных методов градиентного спуска. Напомним, регуляризация - это добавка к функции потерь, которая штрафует за норму весов. Мы будем использовать l2 регуляризацию, таким образом функция потерь приобретает следующий вид:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\dfrac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a_w(x_i) - y_i)^2 + \\dfrac{\\mu}{2} \\| w \\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Допишите класс **BaseDescentReg** в файле `descents.py`.\n",
    "\n",
    "Протестируйте ваше решение в контесте.\n",
    "\n",
    "Вставьте ссылку на успешную посылку:\n",
    "\n",
    "* **BaseDescentReg**: [96960318](https://contest.yandex.ru/contest/54610/run-report/96960318/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Найдите лучшие параметры обучения с регуляризацией аналогично 5 заданию. Будем подбирать длину шага $\\lambda$ (`lambda_`) и коэффициент регуляризации $\\mu$ (`mu`).\n",
    "\n",
    "Сравните для каждого метода результаты с регуляризацией и без регуляризации (нужно опять сохранить ошибку и качество по метрике $R^2$ на обучающей и тестовой выборках и количество итераций до сходимости).\n",
    "\n",
    "Постройте для каждого метода график со значениями функции потерь MSE с регуляризацией и без регуляризации (всего должно получиться 4 графика).\n",
    "\n",
    "Посмотрите на получившиеся результаты. Какие можно сделать выводы, как регуляризация влияет на сходимость? Как изменилось качество на обучающей выборке? На тестовой? Чем вы можете объяснить это?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descent_names = ['full', 'stochastic', 'momentum', 'adam']\n",
    "\n",
    "descent_config = {\n",
    "    'descent_name': 'name',\n",
    "    'regularized': True,\n",
    "    'kwargs': {\n",
    "        'dimension': x_train.shape[1],\n",
    "        'lambda_': 1e-3,\n",
    "        'mu': 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mus = [5, 1, 0, 1e-1, 1e-2, 1e-3, 1e-4]\n",
    "params_pairs = list(product(lambdas, mus))\n",
    "\n",
    "stats_dict_reg = create_stats_dict()\n",
    "\n",
    "for descent_name in descent_names:\n",
    "    descent_config['descent_name'] = descent_name\n",
    "    for lambda_, mu in tqdm(params_pairs):\n",
    "        descent_config['kwargs']['lambda_'] = lambda_\n",
    "        descent_config['kwargs']['mu'] = mu\n",
    "        descent = get_descent(descent_config)\n",
    "\n",
    "        regression = LinearRegression(\n",
    "            descent_config=descent_config\n",
    "        )\n",
    "\n",
    "        fit_and_update_stats(regression, x_train, y_train, x_val, y_val, stats_dict_reg, descent_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_reg = get_best_params(stats_dict_reg, params_pairs, \"lambda_mu\")   \n",
    "results_reg_df = pd.DataFrame(results_reg)\n",
    "results_reg_df[[\"method_name\", \"losses_train\", \"losses_val\", \"r_2_train\", \"r_2_val\", \"iterations\", \"lambda_mu\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[[\"method_name\", \"losses_train\", \"losses_val\", \"r_2_train\", \"r_2_val\", \"iterations\", \"lambda\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(15, 10))\n",
    "descent_names_numed = {\"full\": 0, \"stochastic\": 1, \"momentum\": 2, \"adam\": 3}\n",
    "\n",
    "for res in results:\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].plot(res[\"losses_history\"], label=(res[\"method_name\"] + \" no reg\"))\n",
    "\n",
    "for res in results_reg:\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].plot(res[\"losses_history\"], label=(res[\"method_name\"] + \" with reg\"))\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].legend()\n",
    "\n",
    "plt.suptitle(\"Losses while fitting with regularization and without\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(15, 10))\n",
    "descent_names_numed = {\"full\": 0, \"stochastic\": 1, \"momentum\": 2, \"adam\": 3}\n",
    "\n",
    "for res in results:\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].plot(res[\"losses_history\"][10:], label=(res[\"method_name\"] + \" no reg\"))\n",
    "\n",
    "for res in results_reg:\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].plot(res[\"losses_history\"][10:], label=(res[\"method_name\"] + \" with reg\"))\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].legend()\n",
    "\n",
    "plt.suptitle(\"Losses while fitting with regularization and without (from step 10)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(15, 10))\n",
    "descent_names_numed = {\"full\": 0, \"stochastic\": 1, \"momentum\": 2, \"adam\": 3}\n",
    "\n",
    "for res in results:\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].plot(res[\"losses_history\"][40:], label=(res[\"method_name\"] + \" no reg\"))\n",
    "\n",
    "for res in results_reg:\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].plot(res[\"losses_history\"][40:], label=(res[\"method_name\"] + \" with reg\"))\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].legend()\n",
    "\n",
    "plt.suptitle(\"Losses while fitting with regularization and without (from step 40)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выводы:**\n",
    "\n",
    "1. Cкорость сходимости улучшилась только у Адама \n",
    "2. Остальные методы так и не сошлись за 300 итераций\n",
    "3. Во всех методах градиент быстрее приближался к оптимуму - проще найти нужную \"ямку\", но вот как было сказано в пункте 2, градиенту из-за ограничений сложно сойтись в оптимум\n",
    "4. Блуждания градиента стали более гладкими\n",
    "\n",
    "**Про качество**\n",
    "\n",
    "1. В полном спуске - ухудшилось.\n",
    "2. Адам и (на удивление) стохастический не изменилсь - адам сам по себе делает неплохую регуляризацию\n",
    "3. А вот momentum стал лучше на валидации - скорее всего, было небольшое переобучение\n",
    "\n",
    "**Итог:** в нашем случае для Адама есть смысл использовать регуляризацию для скорости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Задание 8. Альтернативная функция потерь (1 балл)\n",
    "\n",
    "В этом задании вам предстоит использовать другую функцию потерь для нашей задачи регрессии. В качестве функции потерь мы выбрали **Log-Cosh**:\n",
    "\n",
    "$$\n",
    "    L(y, a)\n",
    "    =\n",
    "    \\log\\left(\\cosh(a - y)\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Самостоятельно продифференцируйте данную функцию потерь чтобы найти её градиент:\n",
    "\n",
    "$$\\frac{dL(y, a)}{da} = \\frac{1}{cosh(a - y)} \\cdot sinh(a - y) = tanh(a - y)$$\n",
    "\n",
    "По правилам дифференцирования сложной функции\n",
    "$$\\frac{dL(y, a)}{dw} = (\\frac{da}{dw})^T \\cdot \\frac{dL(y, a)}{da}$$\n",
    "\n",
    "$\\frac{da}{dw}$ - Якобиан \n",
    "\n",
    "$$\\frac{da}{dw} = \\frac{d(Xw)}{dw} = X$$\n",
    "\n",
    "$$\\frac{dL(y, a)}{dw} = X^Ttanh(Xw - y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Программно реализуйте градиентный спуск с данной функцией потерь в файле `descents.py`, обучите все четыре метода (без регуляризации) аналогично 5 заданию, сравните их качество с четырьмя методами из 5 задания.\n",
    "\n",
    "Пример того, как можно запрограммировать использование нескольких функций потерь внутри одного класса градиентного спуска:\n",
    "\n",
    "\n",
    "```python\n",
    "from enum import auto\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LossFunction(Enum):\n",
    "    MSE = auto()\n",
    "    MAE = auto()\n",
    "    LogCosh = auto()\n",
    "    Huber = auto()\n",
    "\n",
    "...\n",
    "class BaseDescent:\n",
    "    def __init__(self, loss_function: LossFunction = LossFunction.MSE):\n",
    "        self.loss_function: LossFunction = loss_function\n",
    "\n",
    "    def calc_gradient(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        if self.loss_function is LossFunction.MSE:\n",
    "            return ...\n",
    "        elif self.loss_function is LossFunction.LogCosh:\n",
    "            return ...\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descents import LossFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "descent_names = ['full', 'stochastic', 'momentum', 'adam']\n",
    "\n",
    "descent_config = {\n",
    "    'descent_name': 'name',\n",
    "    'kwargs': {\n",
    "        'dimension': x_train.shape[1],\n",
    "        'lambda_': 1e-3,\n",
    "        'loss_function': LossFunction.LogCosh\n",
    "    }\n",
    "}\n",
    "\n",
    "stats_dict_logcosh = create_stats_dict()\n",
    "\n",
    "for descent_name in tqdm(descent_names):\n",
    "    descent_config['descent_name'] = descent_name\n",
    "    for lambda_ in lambdas:\n",
    "        descent_config['kwargs']['lambda_'] = lambda_\n",
    "        descent = get_descent(descent_config)\n",
    "\n",
    "        regression = LinearRegression(\n",
    "            descent_config=descent_config\n",
    "        )\n",
    "\n",
    "        fit_and_update_stats(regression, x_train, y_train, x_val, y_val, stats_dict_logcosh, descent_name)\n",
    "\n",
    "        stats_dict_logcosh[descent_name][\"mse_val\"].append(MSE(regression.predict(x_val), y_val))\n",
    "        stats_dict_logcosh[descent_name][\"mse_train\"].append(MSE(regression.predict(x_train), y_train))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_logcosh = get_best_params(stats_dict_logcosh, lambdas, \"lambda\")   \n",
    "results_logcosh_df = pd.DataFrame(results_logcosh)\n",
    "results_logcosh_df[[\"method_name\", \"mse_train\", \"mse_val\", \"losses_train\", \"losses_val\", \"r_2_train\", \"r_2_val\", \"iterations\", \"lambda\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[[\"method_name\", \"losses_train\", \"losses_val\", \"r_2_train\", \"r_2_val\", \"iterations\", \"lambda\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(15, 10))\n",
    "descent_names_numed = {\"full\": 0, \"stochastic\": 1, \"momentum\": 2, \"adam\": 3}\n",
    "\n",
    "for res in results:\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].plot(res[\"losses_history\"], label=(res[\"method_name\"]))\n",
    "\n",
    "for res in results_logcosh:\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].plot(res[\"losses_history\"], label=(res[\"method_name\"] + \" logcosh\"))\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].legend()\n",
    "\n",
    "plt.suptitle(\"Losses while fitting mse and logcosh\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(15, 10))\n",
    "descent_names_numed = {\"full\": 0, \"stochastic\": 1, \"momentum\": 2, \"adam\": 3}\n",
    "\n",
    "for res in results:\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].plot(res[\"losses_history\"][10:], label=(res[\"method_name\"]))\n",
    "\n",
    "for res in results_logcosh:\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].plot(res[\"losses_history\"][10:], label=(res[\"method_name\"] + \" logcosh\"))\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].legend()\n",
    "\n",
    "plt.suptitle(\"Losses while fitting mse and logcosh (from step 10)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(15, 10))\n",
    "descent_names_numed = {\"full\": 0, \"stochastic\": 1, \"momentum\": 2, \"adam\": 3}\n",
    "\n",
    "for res in results:\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].plot(res[\"losses_history\"][25:], label=(res[\"method_name\"]))\n",
    "\n",
    "for res in results_logcosh:\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].plot(res[\"losses_history\"][25:], label=(res[\"method_name\"] + \" logcosh\"))\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].legend()\n",
    "\n",
    "plt.suptitle(\"Losses while fitting mse and logcosh (from step 25)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Кулинарно-социализационный бонус. (0.5 балла).\n",
    "\n",
    "Как мы знаем, осень прекрасная пора, время пробовать новое и делиться теплом с друзьями и близкими. Выберите рецепт, который соответствует вашему настроению, приготовьте выбранное блюдо и угостите хотя бы одного человека им. Кратко опишите ваши впечатления, прикрепите рецепт и фотографии блюда и довольного гостя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моя социализация успешно прошла 5 лет назад, поэтому теперь один прекрасный человек угощает меня. Вот, например, в день решения домашки я ел очень вкусную шакшуку\n",
    "\n",
    "![Шакшука](food_2.jpeg)\n",
    "\n",
    "Но я тоже умею радовать этого человека едой!! Не так давно я готовил лимонный брауни по какому-то английскому рецепту, который я уже и не вспомню, но есть фото\n",
    "\n",
    "![Лимонный брауни](food_1.jpeg)\n",
    "\n",
    "А еще у меня есть канал с моими кулинарнымы опытами, но прикладывать я его не буду("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но раз тут просили рецепт, то у меня есть фирменный авторский рецепт грибного крем-супа\n",
    "\n",
    "1. Шампиньоны - 400г\n",
    "2. Лук - 1шт (средняя)\n",
    "3. Картофель - 1 шт (большая)\n",
    "4. Сливки 20% - 200мл\n",
    "5. Сливочное масло\n",
    "6. Вода\n",
    "7. Багет/гренки\n",
    "\n",
    "Время: 40-80 минут, зависит от ловкости рук\n",
    "\n",
    "**Что делать?**\n",
    "\n",
    "1. Луковичку пополам и полукольцами\n",
    "\n",
    "2. Шампиньоны режем как угодно не мелко и не крупно, чтобы обжарились. Но из серединки вырезаем около 10 красивых кусочков - для украшения\n",
    "\n",
    "3. Картофелину режем небольшими кубиками, чтобы проварилась\n",
    "\n",
    "4. Берем кастрюльку и на маленьком (или чуть выше) огне пассеруем лук (обжариваем до прозрачности или легкой золотистости). В это время можно выполнять пункт 2, если вы очень быстрые\n",
    "\n",
    "5. После этого закидываем шампиньоны и обжариваем пару минут. Нужно смотреть по ним: чем темнее они становятся, тем более сероватым будет суп, кому как нравится. Тут же ставим чайник кипеть\n",
    "\n",
    "6. Закидываем картошку и заливаем водой из чайника, чтобы она покрыла все и чуть еще (порядка 600-700мл хватит)\n",
    "\n",
    "7. Варим 20 минут\n",
    "\n",
    "8. Хорошенько блендерим до однородности. Прям хорошенько\n",
    "\n",
    "9. Вливаем 200мл сливок и перемешиваем\n",
    "\n",
    "10. Ставим снова на огонь и доводим до кипения\n",
    "\n",
    "**А украшение?**\n",
    "\n",
    "1. Топим сливочное масло на маленьком огне\n",
    "\n",
    "2. Раскладываем наши красивые кусочки и сразу же переворачиваем в этом же порядке. Их не надо жарить, 4 секунды с каждой стороны и готово\n",
    "\n",
    "3. Снимаем на тарелочку друг рядом с другом\n",
    "\n",
    "4. В эту же сковороду выкладываем нарезанный кубиками багет\n",
    "\n",
    "5. Хорошенько перемешиваем в сливочном масле, пропитанном шампиньонами, чтобы все впиталось\n",
    "\n",
    "6. Можно чуть увеличить огонь и готовить под крышкой иногда помешивая. Готовность - по вкусу\n",
    "\n",
    "7. Выкладываем!\n",
    "\n",
    "![Супчик](soup.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`### ваш кулинарный опыт тут ###`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Бонус 1. Другие методы градиентного спуска (1 балл).\n",
    "\n",
    "По желанию выберите метод градиентного спуска из следующего списка и самостоятельно реализуйте его в файле `descents.py`. Обучите линейную регрессию с помощью этого метода, сравните с методами из основной части домашнего задания.\n",
    "\n",
    "`AdaMax` (вариация Adam основанная на $l_{\\infty}$), `Nadam` (вариация Adam совмещенная с методом импульса Нестерова), `AMSGrad` (ещё одна вариация Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Бонус 2. Другие функции потерь (1 балл).\n",
    "\n",
    "Аналогично 8 заданию реализуйте две функции потерь - **MAE** и **Huber**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Самостоятельно продифференцируйте данные функции потерь, чтобы найти их градиенты:\n",
    "\n",
    "`### ваше решение тут ###`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Программно реализуйте градиентный спуск с данными функциями потерь в файле `descents.py`, обучите все четыре метода (без регуляризации) аналогично 5 заданию, сравните их качество с четырьмя методами из 5 задания.\n",
    "\n",
    "Сравните между собой *для каждого метода отдельно* качество для разных функций потерь. Какая оказалась лучше? Как вы думаете почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
