{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 8. Альтернативная функция потерь (1 балл)\n",
    "\n",
    "В этом задании вам предстоит использовать другую функцию потерь для нашей задачи регрессии. В качестве функции потерь мы выбрали **Log-Cosh**:\n",
    "\n",
    "$$\n",
    "    L(y, a)\n",
    "    =\n",
    "    \\log\\left(\\cosh(a - y)\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самостоятельно продифференцируйте данную функцию потерь чтобы найти её градиент:\n",
    "\n",
    "$$\\frac{dL(y, a)}{da} = \\frac{1}{cosh(a - y)} \\cdot sinh(a - y) = tanh(a - y)$$\n",
    "\n",
    "По правилам дифференцирования сложной функции\n",
    "$$\\frac{dL(y, a)}{dw} = (\\frac{da}{dw})^T \\cdot \\frac{dL(y, a)}{da}$$\n",
    "\n",
    "$\\frac{da}{dw}$ - Якобиан \n",
    "\n",
    "$$\\frac{da}{dw} = \\frac{d(Xw)}{dw} = X$$\n",
    "\n",
    "$$\\frac{dL(y, a)}{dw} = X^Ttanh(Xw - y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Программно реализуйте градиентный спуск с данной функцией потерь в файле `descents.py`, обучите все четыре метода (без регуляризации) аналогично 5 заданию, сравните их качество с четырьмя методами из 5 задания.\n",
    "\n",
    "Пример того, как можно запрограммировать использование нескольких функций потерь внутри одного класса градиентного спуска:\n",
    "\n",
    "\n",
    "```python\n",
    "from enum import auto\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LossFunction(Enum):\n",
    "    MSE = auto()\n",
    "    MAE = auto()\n",
    "    LogCosh = auto()\n",
    "    Huber = auto()\n",
    "\n",
    "...\n",
    "class BaseDescent:\n",
    "    def __init__(self, loss_function: LossFunction = LossFunction.MSE):\n",
    "        self.loss_function: LossFunction = loss_function\n",
    "\n",
    "    def calc_gradient(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        if self.loss_function is LossFunction.MSE:\n",
    "            return ...\n",
    "        elif self.loss_function is LossFunction.LogCosh:\n",
    "            return ...\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descents import LossFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "descent_names = ['full', 'stochastic', 'momentum', 'adam']\n",
    "\n",
    "descent_config = {\n",
    "    'descent_name': 'name',\n",
    "    'kwargs': {\n",
    "        'dimension': x_train.shape[1],\n",
    "        'lambda_': 1e-3,\n",
    "        'loss_function': LossFunction.LogCosh\n",
    "    }\n",
    "}\n",
    "\n",
    "stats_dict_logcosh = create_stats_dict()\n",
    "\n",
    "for descent_name in tqdm(descent_names):\n",
    "    descent_config['descent_name'] = descent_name\n",
    "    for lambda_ in lambdas:\n",
    "        descent_config['kwargs']['lambda_'] = lambda_\n",
    "        descent = get_descent(descent_config)\n",
    "\n",
    "        regression = LinearRegression(\n",
    "            descent_config=descent_config\n",
    "        )\n",
    "\n",
    "        fit_and_update_stats(regression, x_train, y_train, x_val, y_val, stats_dict_logcosh, descent_name)\n",
    "\n",
    "        stats_dict_logcosh[descent_name][\"mse_val\"].append(MSE(regression.predict(x_val), y_val))\n",
    "        stats_dict_logcosh[descent_name][\"mse_train\"].append(MSE(regression.predict(x_train), y_train))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_logcosh = get_best_params(stats_dict_logcosh, lambdas, \"lambda\")   \n",
    "results_logcosh_df = pd.DataFrame(results_logcosh)\n",
    "results_logcosh_df[[\"method_name\", \"mse_train\", \"mse_val\", \"losses_train\", \"losses_val\", \"r_2_train\", \"r_2_val\", \"iterations\", \"lambda\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[[\"method_name\", \"losses_train\", \"losses_val\", \"r_2_train\", \"r_2_val\", \"iterations\", \"lambda\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(15, 10))\n",
    "descent_names_numed = {\"full\": 0, \"stochastic\": 1, \"momentum\": 2, \"adam\": 3}\n",
    "\n",
    "for res in results:\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].plot(res[\"losses_history\"], label=(res[\"method_name\"]))\n",
    "\n",
    "for res in results_logcosh:\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].plot(res[\"losses_history\"], label=(res[\"method_name\"] + \" logcosh\"))\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].legend()\n",
    "\n",
    "plt.suptitle(\"Losses while fitting mse and logcosh\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(15, 10))\n",
    "descent_names_numed = {\"full\": 0, \"stochastic\": 1, \"momentum\": 2, \"adam\": 3}\n",
    "\n",
    "for res in results:\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].plot(res[\"losses_history\"][10:], label=(res[\"method_name\"]))\n",
    "\n",
    "for res in results_logcosh:\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].plot(res[\"losses_history\"][10:], label=(res[\"method_name\"] + \" logcosh\"))\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].legend()\n",
    "\n",
    "plt.suptitle(\"Losses while fitting mse and logcosh (from step 10)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(15, 10))\n",
    "descent_names_numed = {\"full\": 0, \"stochastic\": 1, \"momentum\": 2, \"adam\": 3}\n",
    "\n",
    "for res in results:\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].plot(res[\"losses_history\"][25:], label=(res[\"method_name\"]))\n",
    "\n",
    "for res in results_logcosh:\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].plot(res[\"losses_history\"][25:], label=(res[\"method_name\"] + \" logcosh\"))\n",
    "    ax[descent_names_numed[res[\"method_name\"]]].legend()\n",
    "\n",
    "plt.suptitle(\"Losses while fitting mse and logcosh (from step 25)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
